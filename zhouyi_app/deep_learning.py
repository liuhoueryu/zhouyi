# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import json
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import sys
import os

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ï¼Œè§£å†³Windowsç¼–ç é—®é¢˜
try:
    if sys.platform == "win32":
        # Windowsç³»ç»Ÿä¸‹çš„ç¼–ç ä¿®å¤
        sys.stdout.reconfigure(encoding='utf-8')
except:
    pass


def safe_print(message):
    """å®‰å…¨çš„æ‰“å°å‡½æ•°ï¼Œé¿å…ç¼–ç é”™è¯¯"""
    try:
        # æ›¿æ¢Unicodeè¡¨æƒ…ä¸ºæ–‡æœ¬æè¿°
        replacements = {
            'ğŸ¤–': '[AI]',
            'ğŸ“Š': '[DATA]',
            'ğŸ”¥': '[TORCH]',
            'ğŸ”§': '[TOOL]',
            'ğŸ¯': '[TARGET]',
            'ğŸ”„': '[PROCESS]',
            'âŒ': '[ERROR]',
            'âœ…': '[OK]',
            'âš ï¸': '[WARN]',
            'ğŸ‰': '[SUCCESS]',
            'ğŸ¤”': '[THINK]',
            'ğŸ’­': '[IDEA]',
            'ğŸŒ±': '[GROW]',
            'ğŸ›¡ï¸': '[PROTECT]',
            'ğŸ”': '[SEARCH]',
            'ğŸš€': '[LAUNCH]',
            'ğŸ“ˆ': '[TREND_UP]',
            'âš–ï¸': '[BALANCE]'
        }

        clean_message = message
        for emoji, text in replacements.items():
            clean_message = clean_message.replace(emoji, text)

        print(clean_message)
    except UnicodeEncodeError:
        # å¦‚æœè¿˜æœ‰ç¼–ç é”™è¯¯ï¼Œä½¿ç”¨ASCIIå®‰å…¨çš„è¾“å‡º
        ascii_message = message.encode('utf-8', 'ignore').decode('utf-8')
        print(ascii_message)


# å®‰å…¨å¯¼å…¥TensorFlow
try:
    import tensorflow as tf

    TF_AVAILABLE = True
    TF_VERSION = tf.__version__
    safe_print("[OK] TensorFlow " + TF_VERSION + " Loaded successfully")
except ImportError as e:
    safe_print("[ERROR] TensorFlow is Unuseful: " + str(e))
    TF_AVAILABLE = False
    TF_VERSION = "æœªå®‰è£…"

# å®‰å…¨å¯¼å…¥PyTorchå’ŒTransformers
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    import torch

    TRANSFORMERS_AVAILABLE = True
    TORCH_VERSION = torch.__version__
    safe_print("[OK] PyTorch " + TORCH_VERSION + " Loaded successfully")
except ImportError as e:
    safe_print("[ERROR] PyTorch/Transformers Unuseful: " + str(e))
    TRANSFORMERS_AVAILABLE = False
    TORCH_VERSION = "æœªå®‰è£…"
try:
    from .ollama_integration import ollama_client

    OLLAMA_AVAILABLE = ollama_client is not None and ollama_client.available
    if OLLAMA_AVAILABLE:
        safe_print("Ollama successful")
    else:
        safe_print("Ollama unuseful")
except ImportError as e:
    safe_print(f"Ollama failure: {e}")
    OLLAMA_AVAILABLE = False


class FortuneAIAnalyzer:
    def __init__(self):
        self.sentiment_analyzer = None
        self.text_generator = None
        self.similarity_model = None
        self.vectorizer = TfidfVectorizer()
        self.similarity_matrix = None
        self.record_texts = []
        self.records = []

        self.versions = {
            'torch': TORCH_VERSION,
            'transformers_available': TRANSFORMERS_AVAILABLE
        }

        self.initialize_models()

    def initialize_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        safe_print("åˆå§‹åŒ–AIæ¨¡å‹...")

        # æƒ…æ„Ÿåˆ†ææ¨¡å‹ - ä½¿ç”¨æ›´å¥½çš„ä¸­æ–‡æ¨¡å‹
        if TRANSFORMERS_AVAILABLE:
            try:
                # å°è¯•åŠ è½½ä¸“é—¨çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹
                self.sentiment_analyzer = pipeline(
                    "sentiment-analysis",
                    framework="pt"
                )
                safe_print("ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ (seamew/roberta-wwm-chinese-text-classification)")
            except Exception as e:
                safe_print(f"ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                try:
                    # å¤‡ç”¨æ¨¡å‹1
                    self.sentiment_analyzer = pipeline(
                        "sentiment-analysis",
                        model="cardiffnlp/twitter-xlm-roberta-base-sentiment",
                        framework="pt"
                    )
                    safe_print("å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ (twitter-xlm-roberta-base-sentiment)")
                except Exception as e2:
                    safe_print(f"å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
                    try:
                        # å¤‡ç”¨æ¨¡å‹2 - ä½¿ç”¨æ›´å°çš„æ¨¡å‹
                        self.sentiment_analyzer = pipeline(
                            "sentiment-analysis",
                            framework="pt"
                        )
                        safe_print("é»˜è®¤æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
                    except Exception as e3:
                        safe_print(f"æ‰€æœ‰æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥: {e3}")
                        self.sentiment_analyzer = None
        else:
            safe_print("Transformersä¸å¯ç”¨ï¼Œè·³è¿‡æƒ…æ„Ÿåˆ†ææ¨¡å‹")

        safe_print("AIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def analyze_sentiment(self, text):
        """åˆ†æç”¨æˆ·è¾“å…¥çš„æƒ…æ„Ÿå€¾å‘ - æ”¹è¿›ç‰ˆæœ¬"""
        if not text or not str(text).strip():
            return {
                "sentiment": "ä¸­æ€§",
                "confidence": 0.5,
                "model_used": "fallback_empty_text"
            }

        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºäºå…³é”®è¯çš„åˆ†æ
        if not self.sentiment_analyzer:
            return self.keyword_based_sentiment_analysis(text)

        try:
            # æ¸…ç†æ–‡æœ¬
            clean_text = str(text).strip()[:500]

            # æ‰§è¡Œæƒ…æ„Ÿåˆ†æ
            result = self.sentiment_analyzer(clean_text)

            sentiment = result[0]['label']
            confidence = result[0]['score']

            safe_print(f"æƒ…æ„Ÿåˆ†æåŸå§‹ç»“æœ: {sentiment}, ç½®ä¿¡åº¦: {confidence}")

            # æ”¹è¿›çš„æ ‡ç­¾æ˜ å°„
            sentiment_map = {
                # äºŒåˆ†ç±»æ¨¡å‹
                'LABEL_0': 'æ¶ˆæ',
                'LABEL_1': 'ç§¯æ',
                'negative': 'æ¶ˆæ',
                'positive': 'ç§¯æ',
                # ä¸‰åˆ†ç±»æ¨¡å‹
                'LABEL_2': 'ä¸­æ€§',  # æœ‰äº›æ¨¡å‹çš„ä¸­æ€§æ ‡ç­¾
                'neutral': 'ä¸­æ€§',
                # å¤šè¯­è¨€æ¨¡å‹
                'Negative': 'æ¶ˆæ',
                'Positive': 'ç§¯æ',
                'Neutral': 'ä¸­æ€§',
                # æ˜Ÿçº§è¯„åˆ†æ¨¡å‹
                '1 star': 'æ¶ˆæ',
                '2 stars': 'æ¶ˆæ',
                '3 stars': 'ä¸­æ€§',
                '4 stars': 'ç§¯æ',
                '5 stars': 'ç§¯æ'
            }

            detected_sentiment = sentiment_map.get(sentiment.lower(), 'ä¸­æ€§')

            # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´ç»“æœ
            if confidence < 0.4:
                # ç½®ä¿¡åº¦å¤ªä½ï¼Œä½¿ç”¨å…³é”®è¯åˆ†æ
                keyword_result = self.keyword_based_sentiment_analysis(text)
                if keyword_result['confidence'] > 0.6:
                    return keyword_result

            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœç½®ä¿¡åº¦ä¸­ç­‰ä½†ç»“æœä¸å…³é”®è¯åˆ†æä¸ä¸€è‡´ï¼Œä½¿ç”¨å…³é”®è¯åˆ†æ
            if 0.4 <= confidence < 0.7:
                keyword_result = self.keyword_based_sentiment_analysis(text)
                if (keyword_result['sentiment'] != detected_sentiment and
                        keyword_result['confidence'] > 0.6):
                    safe_print(f"æ¨¡å‹ä¸å…³é”®è¯åˆ†æä¸ä¸€è‡´ï¼Œä½¿ç”¨å…³é”®è¯ç»“æœ")
                    return keyword_result

            return {
                "sentiment": detected_sentiment,
                "confidence": float(confidence),
                "raw_sentiment": sentiment,
                "model_used": "transformer_model"
            }

        except Exception as e:
            safe_print(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            # å›é€€åˆ°å…³é”®è¯åˆ†æ
            return self.keyword_based_sentiment_analysis(text)

    def keyword_based_sentiment_analysis(self, text):
        """åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if not text:
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5, "model_used": "keyword_fallback"}

        text_lower = str(text).lower()

        # æ‰©å±•çš„æƒ…æ„Ÿå…³é”®è¯åº“
        positive_words = {
            'å¼€å¿ƒ': 2, 'é«˜å…´': 2, 'å¿«ä¹': 2, 'å–œæ‚¦': 2, 'å¹¸ç¦': 2,
            'é¡ºåˆ©': 1, 'æˆåŠŸ': 1, 'å¥½è¿': 1, 'æ»¡æ„': 1, 'å–œæ¬¢': 1,
            'çˆ±': 2, 'å¸Œæœ›': 1, 'æœŸå¾…': 1, 'å…´å¥‹': 2, 'æ¿€åŠ¨': 1,
            'ç¾å¥½': 1, 'å®Œç¾': 1, 'ä¼˜ç§€': 1, 'ç²¾å½©': 1, 'å‰å®³': 1,
            'æ„Ÿè°¢': 1, 'æ„ŸåŠ¨': 1, 'æ¸©æš–': 1, 'å®‰å¿ƒ': 1, 'æ”¾æ¾': 1,
            'å……æ»¡ä¿¡å¿ƒ': 2, 'ä¹è§‚': 2, 'ç§¯æ': 2, 'å‘ä¸Š': 1
        }

        negative_words = {
            'ä¼¤å¿ƒ': 2, 'éš¾è¿‡': 2, 'ç—›è‹¦': 2, 'æ‚²ä¼¤': 2, 'ç»æœ›': 3,
            'å›°éš¾': 1, 'å¤±è´¥': 1, 'é—®é¢˜': 1, 'æ‹…å¿ƒ': 1, 'ç„¦è™‘': 2,
            'å®³æ€•': 2, 'ææƒ§': 2, 'ç´§å¼ ': 1, 'å‹åŠ›': 1, 'çƒ¦æ¼': 1,
            'ç”Ÿæ°”': 2, 'æ„¤æ€’': 2, 'å¤±æœ›': 2, 'æ²®ä¸§': 2, 'éƒé—·': 1,
            'è®¨åŒ': 1, 'æ¨': 2, 'åæ‚”': 1, 'æ„§ç–š': 1, 'è‡ªè´£': 1,
            'æ— åŠ©': 2, 'å­¤ç‹¬': 2, 'å¯‚å¯': 1, 'ç–²æƒ«': 1, 'ç´¯': 1
        }

        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        positive_score = 0
        negative_score = 0

        for word, weight in positive_words.items():
            if word in text_lower:
                positive_score += weight

        for word, weight in negative_words.items():
            if word in text_lower:
                negative_score += weight

        # å†³å®šæƒ…æ„Ÿå€¾å‘
        total_score = positive_score + negative_score

        if total_score == 0:
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5, "model_used": "keyword_no_match"}

        # è®¡ç®—ç½®ä¿¡åº¦
        max_possible_score = max(
            sum(positive_words.values()),
            sum(negative_words.values())
        ) / 10  # å½’ä¸€åŒ–

        confidence = min(total_score / max_possible_score, 0.9)

        if positive_score > negative_score:
            sentiment = "ç§¯æ"
            final_confidence = max(0.6, confidence)
        elif negative_score > positive_score:
            sentiment = "æ¶ˆæ"
            final_confidence = max(0.6, confidence)
        else:
            sentiment = "ä¸­æ€§"
            final_confidence = 0.5

        safe_print(f"å…³é”®è¯åˆ†æ: ç§¯æ{positive_score}, æ¶ˆæ{negative_score}, æƒ…æ„Ÿ{sentiment}")

        return {
            "sentiment": sentiment,
            "confidence": final_confidence,
            "model_used": "keyword_based"
        }

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def predict_fortune_trend(self, hexagram_name, historical_data):
        """é¢„æµ‹å¦è±¡è¶‹åŠ¿ï¼ˆåŸºäºå†å²æ•°æ®ï¼‰"""
        if not historical_data:
            return {"trend": "stable", "confidence": 0.5}

        try:
            # ç®€å•çš„è¶‹åŠ¿åˆ†æ
            hexagram_counts = {}
            for record in historical_data:
                name = record.hexagram_name
                hexagram_counts[name] = hexagram_counts.get(name, 0) + 1

            total = len(historical_data)
            current_hexagram_count = hexagram_counts.get(hexagram_name, 0)
            probability = current_hexagram_count / total if total > 0 else 0

            # åŸºäºæ¦‚ç‡åˆ¤æ–­è¶‹åŠ¿
            if probability > 0.3:
                trend = "rising"
                confidence = min(probability * 2, 0.9)
            elif probability > 0.1:
                trend = "stable"
                confidence = 0.6
            else:
                trend = "emerging"
                confidence = 0.7

            return {
                "trend": trend,
                "confidence": float(confidence),
                "probability": float(probability),
                "total_cases": total
            }
        except Exception as e:
            safe_print(f"è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {e}")
            return {"trend": "stable", "confidence": 0.5}

    def safe_array_check(self, array, threshold=0.1):
        """å®‰å…¨åœ°æ£€æŸ¥æ•°ç»„æ¡ä»¶ï¼Œé¿å…å¸ƒå°”å€¼æ­§ä¹‰"""
        if array is None or len(array) == 0:
            return False

        # å¦‚æœæ˜¯æ ‡é‡ï¼Œç›´æ¥æ¯”è¾ƒ
        if np.isscalar(array):
            return array > threshold

        # å¦‚æœæ˜¯æ•°ç»„ï¼Œä½¿ç”¨any()æˆ–all()
        try:
            # å¯¹äºç›¸ä¼¼åº¦æ•°ç»„ï¼Œæˆ‘ä»¬å…³å¿ƒæ˜¯å¦æœ‰ä»»ä½•å€¼è¶…è¿‡é˜ˆå€¼
            return (array > threshold).any()
        except ValueError as e:
            safe_print("[ERROR] æ•°ç»„æ£€æŸ¥é”™è¯¯: " + str(e))
            return False

    def train_similarity_model(self, records):
        """è®­ç»ƒç›¸ä¼¼åº¦åŒ¹é…æ¨¡å‹ï¼ˆä½¿ç”¨å®‰å…¨çš„æ•°ç»„æ“ä½œï¼‰"""
        if not records:
            return

        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            texts = []
            self.records = list(records)[:100]  # é™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜

            for record in self.records:
                text = f"{record.thing} {record.hexagram_name}"
                texts.append(self.preprocess_text(text))

            # è®­ç»ƒTF-IDFæ¨¡å‹
            if texts:
                self.similarity_matrix = self.vectorizer.fit_transform(texts)
                self.record_texts = texts
                safe_print("[OK] similarity model Training successfulï¼Œå…± " + str(len(texts)) + " æ¡è®°å½•")
        except Exception as e:
            safe_print("[ERROR] similarity model Training failure: " + str(e))

    # def find_similar_cases(self, query, top_k=5):
    #     """æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²æ¡ˆä¾‹ï¼ˆä¿®å¤æ•°ç»„å¸ƒå°”åˆ¤æ–­ï¼‰"""
    #     if (self.similarity_matrix is None or
    #             not hasattr(self, 'records') or
    #             not self.records):
    #         return []
    #
    #     try:
    #         # é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬
    #         processed_query = self.preprocess_text(query)
    #         query_vector = self.vectorizer.transform([processed_query])
    #
    #         # è®¡ç®—ç›¸ä¼¼åº¦ - ä½¿ç”¨å®‰å…¨çš„æ•°ç»„æ“ä½œ
    #         similarity_scores = cosine_similarity(query_vector, self.similarity_matrix)
    #
    #         # ç¡®ä¿æˆ‘ä»¬å¾—åˆ°çš„æ˜¯1Dæ•°ç»„
    #         if hasattr(similarity_scores, 'shape') and len(similarity_scores.shape) > 1:
    #             similarities = similarity_scores.flatten()
    #         else:
    #             similarities = similarity_scores
    #
    #         # å®‰å…¨åœ°è·å–æœ€ç›¸ä¼¼çš„è®°å½•
    #         if len(similarities) == 0:
    #             return []
    #
    #         # ä½¿ç”¨å®‰å…¨çš„æ•°ç»„æ“ä½œ
    #         valid_indices = []
    #         for idx, score in enumerate(similarities):
    #             # å®‰å…¨åœ°æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
    #             if self.safe_array_check(score, 0.1):
    #                 valid_indices.append((idx, score))
    #
    #         # æŒ‰ç›¸ä¼¼åº¦æ’åº
    #         valid_indices.sort(key=lambda x: x[1], reverse=True)
    #         similar_cases = []
    #
    #         for idx, score in valid_indices[:top_k]:
    #             try:
    #                 record = self.records[idx]
    #                 similar_cases.append({
    #                     'record': record,
    #                     'similarity': float(score),
    #                     'thing': record.thing,
    #                     'hexagram_name': record.hexagram_name,
    #                     'created_time': record.created_time
    #                 })
    #             except (IndexError, AttributeError) as e:
    #                 safe_print("[ERROR] å¤„ç†ç›¸ä¼¼æ¡ˆä¾‹æ—¶å‡ºé”™: " + str(e))
    #                 continue
    #
    #         return similar_cases
    #
    #     except Exception as e:
    #         safe_print("[ERROR] ç›¸ä¼¼æ¡ˆä¾‹æŸ¥æ‰¾å¤±è´¥: " + str(e))
    #         return []

    def find_similar_cases(self, query, top_k=5):
        """æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²æ¡ˆä¾‹ï¼ˆç®€åŒ–ä¿®å¤ç‰ˆï¼‰"""
        if (self.similarity_matrix is None or
                not hasattr(self, 'records') or
                not self.records):
            return []

        try:
            # é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬
            processed_query = self.preprocess_text(query)
            query_vector = self.vectorizer.transform([processed_query])

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity_scores = cosine_similarity(query_vector, self.similarity_matrix)

            # ç»Ÿä¸€è½¬æ¢ä¸º1D numpyæ•°ç»„
            similarities = np.array(similarity_scores).flatten()

            # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„ç´¢å¼•
            if len(similarities) == 0:
                safe_print("[ERROR] æ²¡æœ‰ç›¸ä¼¼æ¡ˆä¾‹")
                return []

            # ä½¿ç”¨argsortè·å–æ’åºåçš„ç´¢å¼•
            top_indices = np.argsort(similarities)[::-1][:top_k]

            similar_cases = []
            for idx in top_indices:
                if idx < len(self.records):
                    try:
                        record = self.records[idx]
                        score = similarities[idx]

                        similar_cases.append({
                            'record': record,
                            'similarity': float(score),
                            'thing': record.thing,
                            'hexagram_name': record.hexagram_name,
                            'created_time': record.created_time
                        })
                    except (IndexError, AttributeError) as e:
                        safe_print(f"[ERROR] å¤„ç†æ¡ˆä¾‹ {idx} æ—¶å‡ºé”™: {e}")
                        continue

            return similar_cases

        except Exception as e:
            safe_print("[ERROR] ç›¸ä¼¼æ¡ˆä¾‹æŸ¥æ‰¾å¤±è´¥: " + str(e))
            return []



    def preprocess_text(self, text):
        """å®‰å…¨çš„æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return ""

        try:
            # ç®€å•çš„æ–‡æœ¬æ¸…ç†ï¼Œé¿å…å¤æ‚åˆ†è¯
            text = str(text)
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡
            text = re.sub(r'[^\w\u4e00-\u9fff\s]', '', text)
            return text.strip()
        except Exception as e:
            safe_print("[ERROR] æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: " + str(e))
            return str(text)[:200]  # è¿”å›æˆªæ–­çš„åŸå§‹æ–‡æœ¬

    def generate_hexagram_interpretation(self, hexagram_name, user_thing, sentiment_analysis, changing_lines=None):
        """ç”Ÿæˆä¸ªæ€§åŒ–çš„å¦è±¡è§£é‡Š - ä½¿ç”¨Ollamaå¤§æ¨¡å‹"""
        # ä¼˜å…ˆä½¿ç”¨Ollamaå¤§æ¨¡å‹
        if OLLAMA_AVAILABLE:
            try:
                safe_print("æ­£åœ¨ä½¿ç”¨Ollamaå¤§æ¨¡å‹ç”Ÿæˆè§£é‡Š...")
                interpretation = ollama_client.generate_interpretation(
                    hexagram_name,
                    user_thing,
                    sentiment_analysis,
                    changing_lines
                )
                safe_print("âœ“ Ollamaå¤§æ¨¡å‹è§£é‡Šç”ŸæˆæˆåŠŸ")
                return interpretation
            except Exception as e:
                safe_print(f"Ollamaå¤§æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨è§£é‡Š")
                # å¤±è´¥æ—¶å›é€€åˆ°è§„åˆ™åŸºç¡€è§£é‡Š
                return self.get_fallback_interpretation(hexagram_name, user_thing, sentiment_analysis, changing_lines)
        else:
            # Ollamaä¸å¯ç”¨æ—¶ä½¿ç”¨è§„åˆ™åŸºç¡€è§£é‡Š
            safe_print("ä½¿ç”¨è§„åˆ™åŸºç¡€è§£é‡Š")
            return self.get_fallback_interpretation(hexagram_name, user_thing, sentiment_analysis, changing_lines)

    def get_fallback_interpretation(self, hexagram_name, user_thing, sentiment_analysis):
        """å¤‡ç”¨è§£é‡Šç”Ÿæˆï¼ˆåŸºäºè§„åˆ™ï¼‰"""
        sentiment = sentiment_analysis.get('sentiment', 'ä¸­æ€§')
        confidence = sentiment_analysis.get('confidence', 0.5)

        # åŸºäºå¦è±¡å’Œæƒ…æ„Ÿçš„è§„åˆ™è§£é‡Š
        interpretations = {
            'ä¹¾ä¸ºå¤©': {
                'ç§¯æ': f"[SUCCESS] ä¹¾å¦è±¡å¾å¤©è¡Œå¥ï¼Œå›å­ä»¥è‡ªå¼ºä¸æ¯ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºæ‚¨æ­£å¤„åœ¨ç§¯æå‘ä¸Šçš„é˜¶æ®µï¼Œå®œæŠŠæ¡æ—¶æœºï¼Œå‹‡å¾€ç›´å‰ã€‚",
                'æ¶ˆæ': f"[THINK] ä¹¾å¦è™½å¼ºï¼Œä½†äº¢é¾™æœ‰æ‚”ã€‚å¯¹äº'{user_thing}'ï¼Œå¦è±¡æé†’æ‚¨éœ€æ³¨æ„åˆšæ„è‡ªç”¨ï¼Œé€‚å½“è°ƒæ•´ç­–ç•¥ï¼Œä»¥æŸ”å…‹åˆšã€‚",
                'ä¸­æ€§': f"[BALANCE] ä¹¾å¦ä»£è¡¨åˆ›é€ ä¸é¢†å¯¼ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºå±€åŠ¿å°šä¸æ˜æœ—ï¼Œå»ºè®®æ‚¨ä¿æŒè€å¿ƒï¼Œç­‰å¾…æ›´å¥½çš„æ—¶æœºã€‚"
            },
            'å¤ä¸ºåœ°': {
                'ç§¯æ': f"[GROW] å¤å¦è±¡å¾åœ°åŠ¿å¤ï¼Œå›å­ä»¥åšå¾·è½½ç‰©ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºå®œç¨³æ‰ç¨³æ‰“ï¼Œç§¯ç´¯å®åŠ›ï¼Œç»ˆæœ‰æ‰€æˆã€‚",
                'æ¶ˆæ': f"[IDEA] å¤å¦ä¸»é™ï¼Œå¯¹äº'{user_thing}'çš„æŒ‘æˆ˜ï¼Œå¦è±¡å»ºè®®æ‚¨ä»¥å®ˆä¸ºæ”»ï¼Œç§¯è“„åŠ›é‡ï¼Œä¸å®œè´¸ç„¶è¡ŒåŠ¨ã€‚",
                'ä¸­æ€§': f"[PROCESS] å¤å¦ä»£è¡¨åŒ…å®¹ä¸æ‰¿è½½ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºéœ€è¦æ›´å¤šè€å¿ƒï¼Œé¡ºå…¶è‡ªç„¶ä¼šæœ‰è½¬æœºã€‚"
            },
            'æ°´é›·å±¯': {
                'ç§¯æ': f"[GROW] å±¯å¦è±¡å¾ä¸‡ç‰©åˆç”Ÿã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºè™½ç„¶èµ·æ­¥è‰°éš¾ï¼Œä½†åªè¦åšæŒåŠªåŠ›ï¼Œå¿…èƒ½å¼€åˆ›å±€é¢ã€‚",
                'æ¶ˆæ': f"[PROTECT] å±¯å¦æç¤ºåˆåˆ›ä¹‹éš¾ã€‚å¯¹äº'{user_thing}'ï¼Œå¦è±¡å»ºè®®æ‚¨è°¨æ…è¡Œäº‹ï¼Œæ‰“å¥½åŸºç¡€ï¼Œé¿å…æ€¥äºæ±‚æˆã€‚",
                'ä¸­æ€§': f"[PROCESS] å±¯å¦ä»£è¡¨èµ·å§‹ä¸ç§¯ç´¯ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºéœ€è¦å¾ªåºæ¸è¿›ï¼Œä¸å¯æ“ä¹‹è¿‡æ€¥ã€‚"
            },
            'å·½ä¸ºé£': {
                'ç§¯æ': f"[IDEA] è’™å¦è±¡å¾å¯è’™ä¸å‘å±•ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºéœ€è¦å­¦ä¹ å’Œæ¢ç´¢ï¼Œå°†ä¼šè·å¾—æ–°çš„è®¤çŸ¥å’Œæœºä¼šã€‚",
                'æ¶ˆæ': f"[THINK] è’™å¦æç¤ºè¿·èŒ«ä¹‹è±¡ã€‚å¯¹äº'{user_thing}'ï¼Œå¦è±¡å»ºè®®æ‚¨å¯»æ±‚æŒ‡å¯¼ï¼Œå¤šé—®å¤šå­¦ï¼Œé¿å…ç›²ç›®è¡ŒåŠ¨ã€‚",
                'ä¸­æ€§': f"[SEARCH] è’™å¦ä»£è¡¨æ±‚çŸ¥ä¸å¯å‘ã€‚å…³äº'{user_thing}'ï¼Œå¦è±¡æ˜¾ç¤ºéœ€è¦æ›´å¤šä¿¡æ¯å’Œæ€è€ƒæ‰èƒ½åšå‡ºå†³å®šã€‚"
            },
            # é»˜è®¤è§£é‡Šæ¨¡æ¿
            'default': {
                'ç§¯æ': f"[TARGET] å…³äº'{user_thing}'ï¼Œ{hexagram_name}å¦è±¡æ˜¾ç¤ºç§¯ææ€åŠ¿ã€‚ä¿æŒå½“å‰æ–¹å‘ï¼Œæ³¨é‡å®é™…è¡ŒåŠ¨ã€‚",
                'æ¶ˆæ': f"[PROTECT] é¢å¯¹'{user_thing}'çš„æŒ‘æˆ˜ï¼Œ{hexagram_name}å¦è±¡å»ºè®®è°¨æ…è¡Œäº‹ï¼Œå¤šå¬å–ä»–äººæ„è§ã€‚",
                'ä¸­æ€§': f"[SEARCH] å…³äº'{user_thing}'ï¼Œ{hexagram_name}å¦è±¡æ˜¾ç¤ºéœ€è¦æ›´å¤šè§‚å¯Ÿã€‚ä¿æŒå¼€æ”¾å¿ƒæ€ï¼Œç­‰å¾…æ—¶æœºæˆç†Ÿã€‚"
            }
        }

        # è·å–ç‰¹å®šå¦è±¡çš„è§£é‡Šæˆ–ä½¿ç”¨é»˜è®¤è§£é‡Š
        hexagram_interpretations = interpretations.get(hexagram_name, interpretations['default'])
        interpretation = hexagram_interpretations.get(sentiment, interpretations['default']['ä¸­æ€§'])

        # æ·»åŠ ç½®ä¿¡åº¦è¯´æ˜
        if confidence > 0.7:
            interpretation += " å¦è±¡æ¸…æ™°ï¼Œå¯ä¿¡åº¦è¾ƒé«˜ã€‚"
        elif confidence > 0.4:
            interpretation += " å¦è±¡å°šå¯ï¼Œå»ºè®®ç»“åˆå®é™…æƒ…å†µåˆ¤æ–­ã€‚"
        else:
            interpretation += " å¦è±¡è¾ƒä¸ºéšæ™¦ï¼Œä»…ä¾›å‚è€ƒã€‚"

        return interpretation


# å…¨å±€AIåˆ†æå™¨å®ä¾‹
ai_analyzer = FortuneAIAnalyzer()
